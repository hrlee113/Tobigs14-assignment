{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning assignment - Tobigs14 이혜린"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ▶ Reinforcement Learning Review  \n",
    "  \n",
    "강화학습을 푼다 = 최적의 정책 함수를 찾는 것  \n",
    "→ 에이전트가 특정 환경과 상호작용하여, 선택 가능한 행동들 중 보상을 최대화하는 행동 또는 행동 순서를 학습  \n",
    "  \n",
    "> Agent  ㅡ(Action)ㅡ>  Environment  ㅡ(State, Reward)ㅡ>  Agent ... (반복)  \n",
    "  \n",
    ">Action : Agent가 특정 State에서 취할 수 있는 행동  \n",
    "State : Agent가 Action을 선택하기 위해 받는 정보  \n",
    "Reward : Agent가 특정 Action을 하였을 때 받는 보상  \n",
    "Policy : Agent behavior function. 특정 state에서 action을 골라주는 함수. $\\pi$  \n",
    "Episode : 시작부터 종료까지 에이전트가 거친 (state, action, reward)의 sequence  \n",
    "  \n",
    "## MDP  \n",
    "Marcov Decision Process  \n",
    "t+1번째의 state는 t번째 state 및 action에만 의존함. (현재 기준에서 미래의 상태는 과거의 상태에 의존하지 않음)  \n",
    "> states, actions, transition function, reward function  \n",
    "  \n",
    "> transition function : 특정 상태에서 특정 행동을 했을 때, 다음 번에 도달할 상태들의 확률을 나타낸 것  \n",
    "reward function : t+1번째 reward를 예측하는 함수  \n",
    "  \n",
    "목표는 미래 보상의 합을 최대로 하는 최적 정책 함수를 찾는 것이다. 그러나 현재 행동을 통해 얻어지는 보상 뿐만 아니라 미래에 얻어지는 보상도 다 고려해야 한다  \n",
    "  \n",
    "## Bellman equation  \n",
    "MDP가 주어졌을 때, 최적의 정책 함수를 찾는 가장 기본적인 방법.  \n",
    "$V(s)$ : Value Function(가치함수). 현재의 state에서 출발하여 얻을 수 있는 모든 보상들의 합의 기대값  \n",
    "$\\gamma$ : Discount Rate(할인율). 미래 가치를 현재 가치로 환산하는 느낌. 0에서 1사이의 수로, 미래에 받는 것보단 현재에 보상을 받는 것이 더 좋다는 것을 반영하기 위한 값  \n",
    "  \n",
    "### 1. Value equation (가치 반복)\n",
    "> $V(s) = max \\sum_{s^{'}}T(s,a,s^{'})[R(s,a,s^{'})+\\gamma V(s^{'})]$  \n",
    "현재 state의 가치 = s에서 a를 했을 때 s'로 갈 확률 * [s에서 a를 해서 s'로 갔을 때 보상 + 할인율 * 다음 state 가치] 의 합의 최댓값  \n",
    "  \n",
    "모든 상태에 대해서 재귀식을 수렴할 때까지 수행하고, 가장 최대값을 내놓을 수 있는 a를 선택하면 그 값이 s의 optimal value!\n",
    "또한 이 때의 최적의 정책 함수가 optimal policy  \n",
    "  \n",
    "policy가 optimal 하다고 전제하고 max를 취함 → deterministic\n",
    "  \n",
    "### 2. Policy equation (정책 반복)\n",
    "> 정책 함수 = $\\pi(s)$  \n",
    "정책 평가 & 정책 개선의 두 단계로 이루어짐.  \n",
    "정책 평가 : 정책 함수가 수렴할 때까지 번갈아가면서 수행 → 각 state가 true value로 수렴  \n",
    "정책 개선 : 수렴 후에는 가장 높은 state로 가는 것 선택!  \n",
    "  \n",
    "policy에 따라 value function이 확률적으로 주어지기 때문에 기대값으로 value function을 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-Free Reinforcement Learning  \n",
    "  \n",
    "environment를 제대로 모르기에 action을 취해보고 그 결과로 정책 함수를 학습시켜야 함  \n",
    "  \n",
    "### TD 학습 (Temporal Difference Learning)  \n",
    "정책 평가에서 action을 정책 함수 $\\pi(s)$ 로 치환한 후 평가하는 방법. \n",
    "> $V^{\\pi}(s) ← (1-\\alpha)V^{\\pi}(s) + \\alpha[R(s, \\pi(s), s^{'}) + \\gamma V^{\\pi}(s^{'})]$  \n",
    "지수 이동 평균 이용 (최근 값에 더 가중치를 준다)  \n",
    "  \n",
    "더이상 모델 T가 필요 없음 ~ MODEL FREE!  \n",
    "그러나 어떤 상태가 좋은지는 알지만 ($V^{\\pi}(s)$), 어떤 행동을 해야 그 상태로 가는지는 알 수 없다  \n",
    "  \n",
    "### Exploration (탐사) & Exploitation (개척)  \n",
    "계속 탐사만 하다보면 최적을 놓칠수도 있음. 섞어서 해야함! trade off 조정  \n",
    "  \n",
    "### Q-Learning  (off-policy learning)  \n",
    "\n",
    "> $ Q(s,a) ← (1-\\alpha)Q(s,a) + \\alpha [R(s, a, s^{'}) + \\gamma max_{\\alpha^{'}}Q(s^{'},a^{'})] $  \n",
    "Q function : 각 state에서 어떤 행동 a를 했을 때 기대되는 미래 보상의 총합. action에 대한 평가  \n",
    "  \n",
    "> $ \\pi(s) = argmax_{a}Q(s,a) $  \n",
    "모델을 모르는 상태에서도 최적 정책 함수를 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ▶ Paper Review - \"Playing Atari with Deep Reinforcement Learning\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main features of Model\n",
    "  \n",
    "> 강화학습을 이용해서 고차원의 sensory input으로부터 직접적으로 control policies를 학습한 첫번째 deep learning model  \n",
    "\n",
    "* Q-learning  \n",
    "* CNN  \n",
    "* Stochastic gradient descent  \n",
    "* experience replay memory  \n",
    "* no adjustment of the architecture or hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input  \n",
    "210 x 160 pixel images → 84 x 84 pixel image produced by $\\phi$. raw pixels (DQN 과정에서 $\\phi$에 의해 고정된 길이로 표현됨)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning\n",
    "  \n",
    "### 1. DQN Algorithm (Deep Q-learning)  \n",
    "\n",
    "\n",
    "* CNN 기반 (input layer - first hidden layer - second hidden layer - final hidden layer (fully-connected) - output layer 의 구조)\n",
    "* 매 step마다 agent의 experience인 $e_{t}$를 저장하는 곳인 **experience replay**는 episodes를 **replay memory**에 보냄\n",
    "* 내부 loop가 진행되는 동안 Q-learning updates와 minibatch updates 진행  \n",
    "* 그 후 agent는 $\\epsilon$-greedy policy에 따라 action을 선택, 실행  \n",
    "  \n",
    "  \n",
    "### 2. Randomizing the samples  \n",
    "  \n",
    "  \n",
    "* 강화 학습과 딥러닝의 차이 중 하나는 강화 학습의 데이터는 강한 correlation을 가진다는 것  \n",
    "* sample간 강한 correlation은 비효율을 낳기 때문에 sample을 randomize한다! (updates의 분산도 줄여줌)  \n",
    "  \n",
    "  \n",
    "### 3. Experience replay\n",
    "  \n",
    "* on-policy의 경우 발산 혹은 지역해에 빠질 위험이 있지만 experience replay(off-policy)를 사용함으로써 behavior distribution은 smoothing 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기존 연구와의 차이점  (neural fitted Q-learning , NFQ와 비교)  \n",
    "  \n",
    "NFQ | DQN\n",
    ":-:|:-:\n",
    "batch updates | stochastical gradient updates\n",
    "deep autoencoder를 통해 저차원의 visual input을 사용했고, 그 후 이것을 표현하는 데에 NFQ를 사용 | 처음부터 끝까지 강화학습을 사용 (raw visual input을 변환하고 action value에 직접적으로 연관성이 있는 특성들을 배우는 데까지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments  \n",
    "  \n",
    "* RMSProp  \n",
    "* simple frame-skipping technique (agent는 매 kth frame에 action을 보고 선택)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics  \n",
    "* policy's estimated action-value function $Q$의 average  \n",
    "* average reward 보다 덜 noisy, 더 stable (smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output  \n",
    "the prediceted Q-Values of the individual action for the input state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results  \n",
    "  \n",
    "7개 중 6개은 이전의 결과를 넘어섰고, 7개 중 3개은 사람보다 뛰어났다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 느낀점  \n",
    "1. 고차원의 raw pixel 데이터를 처음부터 끝까지 강화 학습을 사용했다는 점, 또한 randomness를 중간중간에 부여하여 computation loss 및 분산을 줄이고 efficiency를 늘렸다는 것이 인상 깊었다.  \n",
    "2. 전에는 강화 학습과 관련된 자료를 많이 보지 않았어서 data preprocessing 과정과 model training 과정을 뭔가 분리된 각각의 단계로 생각하고 data scaler와 model을 따로 생각하는 게 뭔가 당연했는데, 이 논문을 통해 model training 뿐만 아니라 data preprocessing도 전 단계의 값을 고려하여 축차적으로 진행될 수 있다는 걸 새삼 깨달았다. sample을 각각 독립된 데이터로 생각하지 않고, 시간의 흐름이 있고 서로 강한 상관관계가 있다고 여기는 강화 학습의 특징이 잘 드러나는 부분인 것 같다.  \n",
    "3. 7개 중 3개는 모델이 사람보다 뛰어났다는 것을 보면서 역시 머신러닝의 꽃은 강화학습이라는 생각이,,, 완전 AI AI 하다~\n",
    "4. 언젠가는 구현도 해볼 수 있으면,, 좋겠다 ㅎㅎ  \n",
    "  \n",
    "감사합니다 :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
