{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tobig's 14기 2주차 Optimization 과제\n",
    "### Made by 이지용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent 구현하기\n",
    "\n",
    "### 1) \"...\" 표시되어 있는 빈 칸을 채워주세요  \n",
    "### 2) 강의내용과 코드에 대해 공부한 내용을 적어서 과제를 채워주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>48000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>63000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>76000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  bias  experience  salary\n",
       "0      1     1         0.7   48000\n",
       "1      0     1         1.9   48000\n",
       "2      1     1         2.5   60000\n",
       "3      0     1         4.2   63000\n",
       "4      0     1         6.0   76000"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('assignment_2.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test 데이터 나누기\n",
    "### 데이터셋을 train/test로 나눠주는 메소드  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.iloc[:, 1:], data.iloc[:, 0], test_size=0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 3), (50, 3), (150,), (50,))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling  \n",
    "\n",
    "experience와 salary의 단위, 평균, 분산이 크게 차이나므로 scaler를 사용해 단위를 맞춰줍니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.187893</td>\n",
       "      <td>-1.143335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.185555</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>-0.351795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.629277</td>\n",
       "      <td>-1.341220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.308600</td>\n",
       "      <td>0.043974</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1    0.187893 -1.143335\n",
       "1     1    1.185555  0.043974\n",
       "2     1   -0.310938 -0.351795\n",
       "3     1   -1.629277 -1.341220\n",
       "4     1   -1.308600  0.043974"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경사하강법을 사용할 때는 반드시 모든 특성이 같은 스케일을 갖도록 만들어야 한다. 그래야 수렴하는 데 짧은 시간이 걸린다.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "bias_train = X_train[\"bias\"]\n",
    "bias_train = bias_train.reset_index()[\"bias\"]\n",
    "X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "X_train[\"bias\"] = bias_train\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이때 scaler는 X_train에 fit 해주시고, fit한 scaler를 X_test에 적용시켜줍니다.  \n",
    "똑같이 X_test에다 fit하면 안돼요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias</th>\n",
       "      <th>experience</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.344231</td>\n",
       "      <td>-0.615642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508570</td>\n",
       "      <td>0.307821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.310938</td>\n",
       "      <td>0.571667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.363709</td>\n",
       "      <td>1.956862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.987923</td>\n",
       "      <td>-0.747565</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias  experience    salary\n",
       "0     1   -1.344231 -0.615642\n",
       "1     1    0.508570  0.307821\n",
       "2     1   -0.310938  0.571667\n",
       "3     1    1.363709  1.956862\n",
       "4     1   -0.987923 -0.747565"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_test = X_test[\"bias\"]\n",
    "bias_test = bias_test.reset_index()[\"bias\"]\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "X_test[\"bias\"] = bias_test\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter 개수\n",
    "N = len(X_train.loc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23501448, 0.6370105 , 0.59580049])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 초기 parameter들을 임의로 설정해줍니다.\n",
    "parameters = np.array([random.random() for i in range(N)])\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * LaTeX   \n",
    "\n",
    "Jupyter Notebook은 LaTeX 문법으로 수식 입력을 지원하고 있습니다.  \n",
    "http://triki.net/apps/3466  \n",
    "https://jjycjnmath.tistory.com/117"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function\n",
    "\n",
    "## $p = {1 \\over 1 + e^{-(\\beta_{0} + \\beta_{1}x)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(X, parameters):\n",
    "    z = 0\n",
    "    for i in range(len(parameters)) :\n",
    "        z += X[i] * parameters[i]\n",
    "    p = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.734275759414701"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic(X_train.iloc[1], parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Function\n",
    "\n",
    "Object Function : 목적함수는 Gradient Descent를 통해 최적화 하고자 하는 함수입니다.  \n",
    "로지스틱 회귀의 목적함수를 작성해주세요\n",
    "## $l(p) = -{1 \\over N} \\sum_{i=1}^{n}{[y_{i}log(p_{i}) + (1-y_{i})log(1-p_{i})]}$  \n",
    "  \n",
    "+ 전체 데이터를 사용하는 batch gradient descent의 경우 N으로 나누어 주는 것이 맞다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_i(X, y, parameters) :\n",
    "    p = logistic(X, parameters)                            # 위에서 작성한 함수를 활용하세요\n",
    "    loss = y * np.log(p) + (1-y) * log(1-p)\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(X_set, y_set, parameters) :\n",
    "    loss = 0  \n",
    "    for i in range(X_set.shape[0]):\n",
    "        X = X_set.iloc[i, :]\n",
    "        y = y_set.iloc[i]\n",
    "        p = logistic(X, parameters) \n",
    "        loss += y * np.log(p) + (1-y) * np.log(1-p)\n",
    "    return -loss / X_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0733776087172844"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of Cross Entropy\n",
    "\n",
    "## ${\\partial\\over{\\partial \\theta_j}}l(p)= -{1 \\over N}\\sum_{}^{}{(y_{i}-p_{i})x_{ij}}$  \n",
    "  \n",
    "+ 전체 데이터를 사용하는 batch gradient descent의 경우 N으로 나누어 주는 것이 맞다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_entropy를 theta_j에 대해 미분한 값을 구하는 함수\n",
    "def get_gradient_ij_cross_entropy(X, y, parameters, j):\n",
    "    p = logistic(X, parameters)\n",
    "    gradient = (y-p) * X[j]\n",
    "    return -gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1091482631097553"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gradient_ij_cross_entropy(X_train.iloc[0, :], y_train.iloc[0], parameters, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent  \n",
    "\n",
    "Batch Gradient Descent : 학습 한번에 전체 데이터에 대해서 기울기(=Gradient)를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_bgd(X_train, y_train, parameters) :\n",
    "    gradients = [0 for i in range(len(parameters))]\n",
    "    \n",
    "    for i in range(X_train.shape[0]):\n",
    "        X = X_train.iloc[i, :]\n",
    "        y = y_train.iloc[i]\n",
    "        for j in range(len(parameters)):\n",
    "            gradients[j] += get_gradient_ij_cross_entropy(X, y, parameters,j) / X_train.shape[0]\n",
    "            \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.26154037881092196, 0.10079688431933197, 0.3073212832443908]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_bgd = get_gradients_bgd(X_train, y_train, parameters)\n",
    "gradients_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent  \n",
    "\n",
    "Stochastic Gradient Descent : 학습 한번에 임의의 데이터 하나에 대해서만 기울기(=Gradient)를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradients_sgd(X_train, y, parameters) :\n",
    "    gradients = [0 for i in range(len(parameters))]\n",
    "    r = int(random.random()*X_train.shape[0])\n",
    "    X = X_train.iloc[r, :]\n",
    "    y = y_train.iloc[r]\n",
    "        \n",
    "    for j in range(len(parameters)):\n",
    "        gradients[j] = get_gradient_ij_cross_entropy(X, y, parameters,j)\n",
    "        \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7329621899464898, 0.13771849469037306, 0.8057897045752572]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradients_sgd = get_gradients_sgd(X_train, y_train, parameters)\n",
    "gradients_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update Parameters  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, gradients, learning_rate) :\n",
    "    for i in range(len(parameters)) :\n",
    "        gradients[i] *= learning_rate\n",
    "    parameters -= gradients\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23239907, 0.63600253, 0.59272728])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_parameters(parameters, gradients_bgd, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent  \n",
    "\n",
    "위에서 작성한 함수들을 조합해서 Gradient Descent를 진행하는 함수를 완성해주세요\n",
    "\n",
    "learning_rate = 학습 시 스텝의 크기. 학습률이 너무 크면 학습시간이 적게 걸리지만 global minimum에서 멀어질 수 있고, 학습률이 너무 작으면 학습시간이 많이 걸리고 local minimum으로 갈 확률이 커진다.  \n",
    "max_iter = 최대 반복 횟수  \n",
    "tolerance = 허용오차. 허용오차보다 작아지면 (loss가 수렴하면) 거의 최솟값에 도달한 것이므로 알고리즘을 중지한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, learning_rate=0.01, max_iter=100000, tolerance=0.0001, optimizer=\"bgd\") :\n",
    "    count = 1\n",
    "    point = 100 if optimizer == \"bgd\" else 10000\n",
    "    N = len(X_train.iloc[0])\n",
    "    parameters = np.array([random.random() for i in range(N)])\n",
    "    gradients = [0 for i in range(N)]\n",
    "    loss = 0\n",
    "    \n",
    "    while count < max_iter :\n",
    "        \n",
    "        if optimizer == \"bgd\" :\n",
    "            gradients = get_gradients_bgd(X_train, y_train, parameters)\n",
    "        elif optimizer == \"sgd\" :\n",
    "            gradients = get_gradients_sgd(X_train, y_train, parameters)\n",
    "            # loss, 중단 확인\n",
    "        if count%point == 0 :\n",
    "            new_loss = cross_entropy(X_train, y_train, parameters)\n",
    "            print(count, \"loss: \",new_loss, \"params: \", parameters, \"gradients: \", gradients)\n",
    "            \n",
    "            #중단 조건\n",
    "            if abs(new_loss-loss) < tolerance : #tolerance를 len(y_train)으로 나누지는 않았습니다! (시간이 너무 오래걸려서요 ㅜㅜ)\n",
    "                break\n",
    "            loss = new_loss\n",
    "                \n",
    "            \n",
    "                \n",
    "        parameters = update_parameters(parameters, gradients, learning_rate)\n",
    "        count += 1\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  0.6610136832832992 params:  [-0.17980118  0.30967514  0.16991084] gradients:  [0.17693611910162732, -0.015512830017243275, 0.18972904473839206]\n",
      "200 loss:  0.6060316621867174 params:  [-0.33702844  0.33794688 -0.00108748] gradients:  [0.13865477064885207, -0.03959224728377187, 0.15370335976206226]\n",
      "300 loss:  0.5687923807669935 params:  [-0.46038231  0.38558391 -0.14122876] gradients:  [0.10910902838634964, -0.054495684552909734, 0.12785224046892288]\n",
      "400 loss:  0.5417272876101894 params:  [-0.55790586  0.44461183 -0.2596124 ] gradients:  [0.08676087264192894, -0.06275484567437424, 0.10980785154785537]\n",
      "500 loss:  0.5207518549087546 params:  [-0.63591166  0.50962035 -0.36275119] gradients:  [0.06985949208715926, -0.06675208439206008, 0.097046995281208]\n",
      "600 loss:  0.503671047012797 params:  [-0.6991142   0.57722828 -0.45495815] gradients:  [0.05699206173327469, -0.06814673490930157, 0.08773179862665158]\n",
      "700 loss:  0.48925524696121975 params:  [-0.75100318  0.6453942  -0.53904169] gradients:  [0.047115026016881244, -0.06798788777765263, 0.080665995097692]\n",
      "800 loss:  0.4767758724305794 params:  [-0.79417428  0.71290965 -0.61684867] gradients:  [0.03947192671980855, -0.06691932654182124, 0.07509521236176776]\n",
      "900 loss:  0.46577554501114204 params:  [-0.83057475  0.77907552 -0.68962079] gradients:  [0.033512655660095356, -0.06533404257589945, 0.07054416247727474]\n",
      "1000 loss:  0.4559510048930082 params:  [-0.86167831  0.8435039  -0.75821669] gradients:  [0.028833333305940165, -0.06347254904446638, 0.06670982872730676]\n",
      "1100 loss:  0.4470908917163438 params:  [-0.88860926  0.90599762 -0.82324886] gradients:  [0.025134294253242054, -0.061482454512750584, 0.06339563081434885]\n",
      "1200 loss:  0.43904120398331437 params:  [-0.91223144  0.96647658 -0.8851687 ] gradients:  [0.02219099245555116, -0.05945424505796992, 0.06047144357635856]\n",
      "1300 loss:  0.431685313632026 params:  [-0.93321265  1.02493218 -0.94431985] gradients:  [0.019833692448592583, -0.05744291687811045, 0.05784923248428711]\n",
      "1400 loss:  0.42493194899112036 params:  [-0.95207203  1.08139897 -1.00097233] gradients:  [0.017933109260025053, -0.055481213656218574, 0.05546804417011959]\n",
      "1500 loss:  0.4187077042420478 params:  [-0.96921504  1.13593675 -1.05534456] gradients:  [0.016390115804947748, -0.05358783543674501, 0.05328463330915637]\n",
      "1600 loss:  0.41295220796044724 params:  [-0.98495981  1.18861929 -1.10761795] gradients:  [0.01512827086762029, -0.05177259364874392, 0.05126752496040318]\n",
      "1700 loss:  0.4076149026449398 params:  [-0.99955692  1.23952717 -1.15794661] gradients:  [0.014088331802143332, -0.05003968392301351, 0.0493931998328853]\n",
      "1800 loss:  0.4026528288917185 params:  [-1.01320449  1.28874327 -1.20646401] gradients:  [0.013224183269185173, -0.048389782385496696, 0.047643610031241936]\n",
      "1900 loss:  0.39802905369443436 params:  [-1.02605973  1.33635001 -1.25328758] gradients:  [0.012499789166473025, -0.04682139712941586, 0.046004540058462336]\n",
      "2000 loss:  0.3937115230368901 params:  [-1.03824785  1.38242761 -1.29852198] gradients:  [0.011886892401538288, -0.04533174291211322, 0.04446451169075024]\n",
      "2100 loss:  0.38967220151154325 params:  [-1.0498689   1.42705309 -1.3422614 ] gradients:  [0.011363266879607908, -0.043917307780215006, 0.043014042938836684]\n",
      "2200 loss:  0.3858864112673108 params:  [-1.06100314  1.47029979 -1.38459134] gradients:  [0.010911380986167542, -0.04257421909488493, 0.04164514005263894]\n",
      "2300 loss:  0.38233231298359144 params:  [-1.07171516  1.51223708 -1.42558989] gradients:  [0.010517370190572776, -0.04129847815409891, 0.0403509444685181]\n",
      "2400 loss:  0.3789904905766916 params:  [-1.08205719  1.55293031 -1.46532873] gradients:  [0.01017024353027309, -0.040086108379854185, 0.03912548377647794]\n",
      "2500 loss:  0.3758436134647209 params:  [-1.09207162  1.59244095 -1.50387394] gradients:  [0.009861268167614196, -0.038933246526620544, 0.03796349319127967]\n",
      "2600 loss:  0.3728761580926689 params:  [-1.10179304  1.63082666 -1.54128663] gradients:  [0.009583490284205253, -0.037836196336641925, 0.0368602852791511]\n",
      "2700 loss:  0.37007417563575756 params:  [-1.11124979  1.66814149 -1.57762346] gradients:  [0.009331360873800633, -0.03679145752330836, 0.03581165305493251]\n",
      "2800 loss:  0.3674250963219085 params:  [-1.12046524  1.70443611 -1.61293709] gradients:  [0.009100442596982743, -0.03579573866029085, 0.03481379641660143]\n",
      "2900 loss:  0.3649175632405619 params:  [-1.12945877  1.73975801 -1.64727657] gradients:  [0.008887179521103203, -0.03484595970455983, 0.03386326510534909]\n",
      "3000 loss:  0.3625412902101082 params:  [-1.13824658  1.77415167 -1.68068763] gradients:  [0.00868871581443924, -0.03393924798316064, 0.032956913532240784]\n",
      "3100 loss:  0.36028693949848745 params:  [-1.14684229  1.80765883 -1.71321302] gradients:  [0.008502752669066778, -0.03307293020264235, 0.03209186426009092]\n",
      "3200 loss:  0.35814601608516655 params:  [-1.15525744  1.84031864 -1.7448927 ] gradients:  [0.0083274351615066, -0.032244522185458, 0.03126547790815092]\n",
      "3300 loss:  0.35611077581862993 params:  [-1.16350191  1.87216785 -1.77576413] gradients:  [0.008161262618918943, -0.03145171746123217, 0.030475327912915174]\n",
      "3400 loss:  0.35417414532874125 params:  [-1.17158423  1.90324099 -1.80586242] gradients:  [0.008003017484301144, -0.03069237545118966, 0.029719179033566297]\n",
      "3500 loss:  0.3523296519430711 params:  [-1.17951181  1.93357053 -1.83522054] gradients:  [0.007851708772257097, -0.029964509720497055, 0.02899496880370271]\n",
      "3600 loss:  0.35057136216153545 params:  [-1.18729117  1.96318702 -1.86386946] gradients:  [0.007706527055876452, -0.029266276595188403, 0.02830079134772546]\n",
      "3700 loss:  0.34889382748604225 params:  [-1.1949281   1.99211923 -1.89183834] gradients:  [0.007566808583845267, -0.028595964320341314, 0.02763488313138358]\n",
      "3800 loss:  0.34729203659655894 params:  [-1.2024278   2.02039429 -1.91915463] gradients:  [0.007432006639373691, -0.027951982855776446, 0.026995610322207465]\n",
      "3900 loss:  0.3457613730231308 params:  [-1.20979493  2.04803779 -1.9458442 ] gradients:  [0.007301668652423789, -0.027332854352199204, 0.026381457510962728]\n",
      "4000 loss:  0.34429757759293195 params:  [-1.21703378  2.0750739  -1.97193147] gradients:  [0.007175417889582577, -0.02673720431581448, 0.025791017599383913]\n",
      "4100 loss:  0.3428967150384488 params:  [-1.22414827  2.10152545 -1.9974395 ] gradients:  [0.007052938791290122, -0.026163753447175378, 0.025222982698800293]\n",
      "4200 loss:  0.3415551442418632 params:  [-1.23114203  2.12741404 -2.02239008] gradients:  [0.006933965218994762, -0.025611310126321375, 0.024676135913325115]\n",
      "4300 loss:  0.34026949166516357 params:  [-1.23801846  2.15276012 -2.04680382] gradients:  [0.006818271026717085, -0.025078763508380935, 0.024149343903076728]\n",
      "4400 loss:  0.3390366275781059 params:  [-1.24478074  2.17758308 -2.07070027] gradients:  [0.006705662491400978, -0.02456507718982827, 0.02364155013956235]\n",
      "4500 loss:  0.337853644749046 params:  [-1.25143187  2.20190129 -2.09409792] gradients:  [0.0065959722312367575, -0.02406928340419123, 0.023151768778325307]\n",
      "4600 loss:  0.3367178393085123 params:  [-1.25797469  2.22573218 -2.11701432] gradients:  [0.006489054316224608, -0.023590477706296333, 0.02267907908426275]\n",
      "4700 loss:  0.33562669353361185 params:  [-1.26441192  2.24909233 -2.13946613] gradients:  [0.006384780334845922, -0.023127814105490654, 0.022222620353346625]\n",
      "4800 loss:  0.33457786033397724 params:  [-1.27074613  2.27199748 -2.16146918] gradients:  [0.006283036228049715, -0.02268050061026543, 0.021781587281351246]\n",
      "4900 loss:  0.3335691492479207 params:  [-1.27697981  2.2944626  -2.18303853] gradients:  [0.006183719739452244, -0.02224779514906008, 0.021355225735919376]\n",
      "5000 loss:  0.3325985137815009 params:  [-1.28311533  2.31650196 -2.20418848] gradients:  [0.006086738360691709, -0.021829001834492552, 0.020942828893196045]\n",
      "5100 loss:  0.3316640399438928 params:  [-1.28915499  2.33812915 -2.22493268] gradients:  [0.005992007674846859, -0.021423467540814697, 0.02054373370442416]\n",
      "5200 loss:  0.3307639358503314 params:  [-1.295101    2.35935711 -2.24528411] gradients:  [0.0058994500199930095, -0.021030578766829188, 0.020157317661555745]\n",
      "5300 loss:  0.32989652227937544 params:  [-1.30095551  2.3801982  -2.26525515] gradients:  [0.005808993410298701, -0.020649758758882034, 0.019782995834100432]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5400 loss:  0.32906022408466207 params:  [-1.30672056  2.40066422 -2.28485763] gradients:  [0.0057205706643429725, -0.020280464870758883, 0.019420218152247083]\n",
      "5500 loss:  0.3282535623729943 params:  [-1.31239818  2.42076644 -2.30410283] gradients:  [0.005634118700184003, -0.019922186139381508, 0.01906846691378284]\n",
      "5600 loss:  0.3274751473707747 params:  [-1.31799029  2.44051563 -2.32300154] gradients:  [0.0055495779646090974, -0.019574441057124862, 0.018727254494538197]\n",
      "5700 loss:  0.32672367190967005 params:  [-1.32349879  2.4599221  -2.34156406] gradients:  [0.005466891970350074, -0.019236775523326317, 0.0183961212440659]\n",
      "5800 loss:  0.3259979054701648 params:  [-1.3289255   2.47899573 -2.35980026] gradients:  [0.005386006920149981, -0.01890876095916604, 0.01807463355003417]\n",
      "5900 loss:  0.32529668872845496 params:  [-1.33427219  2.49774594 -2.37771958] gradients:  [0.005306871400678162, -0.018589992571571873, 0.017762382056385466]\n",
      "6000 loss:  0.3246189285581033 params:  [-1.33954059  2.51618181 -2.39533106] gradients:  [0.005229436132596285, -0.01828008775313006, 0.017458980021745826]\n",
      "6100 loss:  0.3239635934431407 params:  [-1.34473238  2.53431201 -2.41264336] gradients:  [0.005153653765744956, -0.01797868460618951, 0.017164061805846766]\n",
      "6200 loss:  0.3233297092639005 params:  [-1.34984918  2.55214487 -2.4296648 ] gradients:  [0.00507947871056808, -0.017685440580453262, 0.016877281472866858]\n",
      "6300 loss:  0.32271635542097676 params:  [-1.35489259  2.56968839 -2.44640335] gradients:  [0.005006866998620685, -0.017400031214332517, 0.016598311501645965]\n",
      "6400 loss:  0.32212266126627814 params:  [-1.35986414  2.58695026 -2.46286667] gradients:  [0.004935776166413417, -0.017122148971243017, 0.01632684159365011]\n",
      "6500 loss:  0.32154780281336126 params:  [-1.36476534  2.60393785 -2.47906212] gradients:  [0.004866165157958978, -0.016851502162833064, 0.01606257757041354]\n",
      "6600 loss:  0.32099099970203904 params:  [-1.36959764  2.62065825 -2.49499675] gradients:  [0.00479799424230793, -0.016587813951864868, 0.01580524035294266]\n",
      "6700 loss:  0.3204515123947757 params:  [-1.37436247  2.63711831 -2.51067736] gradients:  [0.004731224943088973, -0.016330821428131383, 0.01555456501625481]\n",
      "6800 loss:  0.31992863958461915 params:  [-1.3790612   2.65332458 -2.52611049] gradients:  [0.004665819977660908, -0.01608027475139966, 0.015310299912837569]\n",
      "6900 loss:  0.319421715796393 params:  [-1.3836952   2.6692834  -2.54130243] gradients:  [0.0046017432039612465, -0.015835936355898615, 0.015072205859385087]\n",
      "7000 loss:  0.3189301091646582 params:  [-1.38826576  2.68500086 -2.55625923] gradients:  [0.004538959573520159, -0.015597580211372917, 0.014840055381663286]\n",
      "7100 loss:  0.3184532193735243 params:  [-1.39277416  2.70048284 -2.57098674] gradients:  [0.00447743508941922, -0.015364991136159132, 0.014613632012819925]\n",
      "7200 loss:  0.31799047574480627 params:  [-1.39722164  2.715735   -2.58549057] gradients:  [0.0044171367682217695, -0.015137964158138662, 0.014392729640873049]\n",
      "7300 loss:  0.31754133546229724 params:  [-1.40160943  2.7307628  -2.59977616] gradients:  [0.004358032605104409, -0.01491630391980038, 0.014177151901470703]\n",
      "7400 loss:  0.31710528192104626 params:  [-1.40593868  2.74557152 -2.61384872] gradients:  [0.004300091541576049, -0.01469982412395165, 0.013966711612377334]\n",
      "7500 loss:  0.3166818231915637 params:  [-1.41021056  2.76016626 -2.62771331] gradients:  [0.004243283435303435, -0.014488347016934898, 0.013761230246429428]\n",
      "7600 loss:  0.31627049058977824 params:  [-1.41442618  2.77455192 -2.64137481] gradients:  [0.004187579031663383, -0.01428170290646333, 0.013560537439996205]\n",
      "7700 loss:  0.31587083734440424 params:  [-1.41858663  2.78873326 -2.65483791] gradients:  [0.004132949936724708, -0.014079729711443572, 0.013364470534223925]\n",
      "7800 loss:  0.3154824373541056 params:  [-1.42269297  2.80271488 -2.66810718] gradients:  [0.0040793685914263245, -0.01388227254136521, 0.013172874146585299]\n",
      "7900 loss:  0.31510488402752534 params:  [-1.42674624  2.81650122 -2.681187  ] gradients:  [0.004026808246773699, -0.013689183303054596, 0.012985599770445807]\n",
      "8000 loss:  0.31473778919983936 params:  [-1.43074744  2.83009657 -2.69408162] gradients:  [0.003975242939913604, -0.013500320332754659, 0.01280250540057068]\n",
      "8100 loss:  0.31438078212004955 params:  [-1.43469756  2.84350509 -2.70679517] gradients:  [0.003924647470980857, -0.013315548051678481, 0.012623455182648242]\n",
      "8200 loss:  0.31403350850371353 params:  [-1.43859755  2.85673081 -2.71933161] gradients:  [0.0038749973806363287, -0.013134736643321801, 0.012448319085078471]\n",
      "8300 loss:  0.3136956296462629 params:  [-1.44244835  2.86977763 -2.7316948 ] gradients:  [0.003826268928234498, -0.012957761750968202, 0.01227697259140741]\n",
      "8400 loss:  0.3133668215924661 params:  [-1.44625087  2.88264932 -2.74388847] gradients:  [0.0037784390705732896, -0.012784504193939318, 0.012109296411928611]\n",
      "8500 loss:  0.31304677435795036 params:  [-1.450006    2.89534955 -2.75591624] gradients:  [0.0037314854411954466, -0.01261484970126632, 0.01194517621307859]\n",
      "8600 loss:  0.3127351911990474 params:  [-1.45371459  2.90788187 -2.7677816 ] gradients:  [0.003685386330210747, -0.012448688661552815, 0.011784502363378833]\n",
      "8700 loss:  0.3124317879275089 params:  [-1.4573775   2.92024971 -2.77948795] gradients:  [0.0036401206646239637, -0.01228591588790719, 0.01162716969475942]\n",
      "8800 loss:  0.3121362922669369 params:  [-1.46099555  2.93245641 -2.79103857] gradients:  [0.0035956679891541807, -0.012126430396901009, 0.011473077278200178]\n",
      "8900 loss:  0.3118484432480085 params:  [-1.46456954  2.94450521 -2.80243667] gradients:  [0.0035520084475328213, -0.011970135200596333, 0.011322128212704967]\n",
      "9000 loss:  0.311567990639815 params:  [-1.46810026  2.95639926 -2.81368534] gradients:  [0.0035091227642747597, -0.011816937110751718, 0.011174229426702617]\n",
      "9100 loss:  0.31129469441483115 params:  [-1.47158847  2.9681416  -2.82478758] gradients:  [0.0034669922269149256, -0.011666746554395818, 0.011029291491028142]\n",
      "9200 loss:  0.311028324245241 params:  [-1.47503491  2.97973521 -2.83574632] gradients:  [0.0034255986687063648, -0.011519477400004155, 0.010887228442716427]\n",
      "9300 loss:  0.310768659028504 params:  [-1.47844031  2.99118296 -2.84656438] gradients:  [0.0033849244517718197, -0.011375046793579242, 0.010747957618890363]\n",
      "9400 loss:  0.3105154864402133 params:  [-1.48180539  3.00248765 -2.85724452] gradients:  [0.003344952450709031, -0.011233375003991682, 0.010611399500072142]\n",
      "9500 loss:  0.31026860251244387 params:  [-1.48513084  3.01365201 -2.86778941] gradients:  [0.0033056660366411836, -0.011094385276974096, 0.010477477562310543]\n",
      "9600 loss:  0.31002781123591977 params:  [-1.48841734  3.02467867 -2.87820165] gradients:  [0.003267049061708857, -0.010958003697215076, 0.010346118137551121]\n",
      "9700 loss:  0.3097929241844589 params:  [-1.49166554  3.03557021 -2.88848378] gradients:  [0.003229085843999428, -0.010824159058034848, 0.010217250281720207]\n",
      "9800 loss:  0.3095637601602568 params:  [-1.4948761   3.04632914 -2.89863824] gradients:  [0.003191761152907543, -0.010692782738162536, 0.010090805650034215]\n",
      "9900 loss:  0.30934014485868644 params:  [-1.49804964  3.05695788 -2.90866742] gradients:  [0.003155060194919809, -0.01056380858517114, 0.009966718379077054]\n",
      "10000 loss:  0.3091219105513773 params:  [-1.50118679  3.06745881 -2.91857367] gradients:  [0.0031189685998189915, -0.010437172805153055, 0.009844924975225195]\n",
      "10100 loss:  0.3089088957864297 params:  [-1.50428814  3.07783424 -2.92835923] gradients:  [0.003083472407300521, -0.010312813858255459, 0.00972536420902258]\n",
      "10200 loss:  0.30870094510469875 params:  [-1.50735428  3.08808641 -2.9380263 ] gradients:  [0.0030485580539920004, -0.01019067235971309, 0.009607977015144754]\n",
      "10300 loss:  0.3084979087711617 params:  [-1.51038579  3.09821752 -2.94757705] gradients:  [0.0030142123608717505, -0.010070690986045435, 0.00949270639760965]\n",
      "10400 loss:  0.3082996425204406 params:  [-1.51338323  3.10822969 -2.95701355] gradients:  [0.0029804225210755195, -0.009952814386111833, 0.009379497339914064]\n",
      "10500 loss:  0.30810600731563165 params:  [-1.51634715  3.118125   -2.96633783] gradients:  [0.0029471760880846144, -0.009836989096726452, 0.009268296719809234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10600 loss:  0.3079168691196356 params:  [-1.51927809  3.12790548 -2.9755519 ] gradients:  [0.0029144609642860527, -0.009723163462571623, 0.009159053228430384]\n",
      "10700 loss:  0.3077320986782444 params:  [-1.52217657  3.1375731  -2.98465766] gradients:  [0.0028822653898970055, -0.009611287560152195, 0.009051717293528756]\n",
      "10800 loss:  0.3075515713142946 params:  [-1.52504311  3.1471298  -2.99365701] gradients:  [0.002850577932244602, -0.00950131312555471, 0.008946241006566736]\n",
      "10900 loss:  0.3073751667322343 params:  [-1.52787821  3.15657744 -3.00255179] gradients:  [0.0028193874753912364, -0.009393193485796512, 0.008842578053446916]\n",
      "11000 loss:  0.3072027688324979 params:  [-1.53068235  3.16591786 -3.01134379] gradients:  [0.002788683210097136, -0.009286883493553345, 0.008740683648671935]\n",
      "11100 loss:  0.3070342655351266 params:  [-1.53345603  3.17515285 -3.02003475] gradients:  [0.002758454624112139, -0.00918233946507756, 0.008640514472733632]\n",
      "11200 loss:  0.3068695486121027 params:  [-1.53619972  3.18428415 -3.02862637] gradients:  [0.0027286914927850646, -0.009079519121122415, 0.008542028612554466]\n",
      "11300 loss:  0.30670851352790146 params:  [-1.53891386  3.19331347 -3.03712033] gradients:  [0.002699383869983858, -0.008978381530709989, 0.008445185504802494]\n",
      "11400 loss:  0.30655105928780235 params:  [-1.54159893  3.20224246 -3.04551824] gradients:  [0.002670522079317057, -0.008878887057578227, 0.008349945881926757]\n",
      "11500 loss:  0.3063970882935203 params:  [-1.54425534  3.21107276 -3.05382168] gradients:  [0.002642096705646386, -0.008780997309165768, 0.00825627172075566]\n",
      "11600 loss:  0.306246506205757 params:  [-1.54688354  3.21980595 -3.06203222] gradients:  [0.0026140985868845033, -0.008684675087988923, 0.008164126193524546]\n",
      "11700 loss:  0.30609922181328225 params:  [-1.54948396  3.22844358 -3.07015135] gradients:  [0.0025865188060653858, -0.008589884345288248, 0.008073473621192569]\n",
      "11800 loss:  0.30595514690819936 params:  [-1.55205699  3.23698716 -3.07818055] gradients:  [0.0025593486836821694, -0.008496590136815675, 0.007984279428931774]\n",
      "11900 loss:  0.3058141961670499 params:  [-1.55460306  3.24543817 -3.08612127] gradients:  [0.0025325797702823154, -0.008404758580650838, 0.007896510103668406]\n",
      "12000 loss:  0.3056762870374489 params:  [-1.55712255  3.25379806 -3.09397491] gradients:  [0.0025062038393124267, -0.008314356816940656, 0.007810133153565309]\n",
      "12100 loss:  0.3055413396299519 params:  [-1.55961585  3.26206825 -3.10174284] gradients:  [0.0024802128802031867, -0.008225352969456253, 0.007725117069347786]\n",
      "12200 loss:  0.30540927661487927 params:  [-1.56208336  3.27025011 -3.10942643] gradients:  [0.002454599091689837, -0.008137716108877233, 0.0076414312873698854]\n",
      "12300 loss:  0.3052800231238317 params:  [-1.56452543  3.278345   -3.11702697] gradients:  [0.002429354875356812, -0.008051416217707974, 0.0075590461543371896]\n",
      "12400 loss:  0.3051535066556589 params:  [-1.56694244  3.28635423 -3.12454576] gradients:  [0.0024044728294007247, -0.007966424156747113, 0.007477932893594208]\n",
      "12500 loss:  0.30502965698664114 params:  [-1.56933474  3.29427911 -3.13198406] gradients:  [0.002379945742604251, -0.007882711633028571, 0.007398063572898815]\n",
      "12600 loss:  0.3049084060846763 params:  [-1.57170269  3.30212091 -3.13934309] gradients:  [0.0023557665885141597, -0.00780025116915726, 0.00731941107360922]\n",
      "12700 loss:  0.30478968802725837 params:  [-1.57404663  3.30988084 -3.14662406] gradients:  [0.0023319285198156498, -0.0077190160739735066, 0.007241949061208019]\n",
      "12800 loss:  0.3046734389230603 params:  [-1.57636689  3.31756014 -3.15382814] gradients:  [0.0023084248628963233, -0.007638980414475094, 0.007165651957100635]\n",
      "12900 loss:  0.30455959683693806 params:  [-1.57866382  3.32515999 -3.1609565 ] gradients:  [0.0022852491125940153, -0.00756011898893664, 0.007090494911621865]\n",
      "13000 loss:  0.3044481017181831 params:  [-1.58093773  3.33268155 -3.16801025] gradients:  [0.0022623949271218437, -0.007482407301168872, 0.0070164537781891145]\n",
      "13100 loss:  0.3043388953318625 params:  [-1.58318894  3.34012595 -3.1749905 ] gradients:  [0.002239856123162806, -0.007405821535857811, 0.006943505088550283]\n",
      "13200 loss:  0.3042319211930926 params:  [-1.58541777  3.34749432 -3.18189834] gradients:  [0.0022176266711314858, -0.0073303385349346925, 0.006871626029068146]\n",
      "13300 loss:  0.3041271245041067 params:  [-1.58762452  3.35478774 -3.18873482] gradients:  [0.0021957006905922753, -0.007255935774924758, 0.006800794417994875]\n",
      "13400 loss:  0.30402445209397283 params:  [-1.58980949  3.36200728 -3.19550097] gradients:  [0.002174072445833438, -0.007182591345229073, 0.006730988683685484]\n",
      "13500 loss:  0.30392385236083913 params:  [-1.59197297  3.369154   -3.20219782] gradients:  [0.002152736341587539, -0.007110283927293986, 0.006662187843707516]\n",
      "13600 loss:  0.30382527521658526 params:  [-1.59411527  3.37622891 -3.20882636] gradients:  [0.0021316869188951777, -0.007038992774627173, 0.0065943714848034594]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.59411527,  3.37622891, -3.20882636])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd = gradient_descent(X_train, y_train)\n",
    "new_param_bgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning\n",
    "\n",
    "Hyper Parameter들을 매번 다르게 해서 학습을 진행해 보세요. 다른 점들을 발견할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loss:  0.3082366151209839 params:  [-1.52177894  3.09306755 -2.99082588] gradients:  [0.29818306047707704, -0.37957791553096276, -0.45893496405346057]\n",
      "20000 loss:  0.2998456524311901 params:  [-1.75576061  3.8278484  -3.56701914] gradients:  [0.48816113452001186, -0.4822656519073101, -0.751331119765138]\n",
      "30000 loss:  0.2986904749333766 params:  [-1.80062944  4.06156537 -3.78809866] gradients:  [0.09161285057045816, 0.10534785341865044, 0.1248872848875075]\n",
      "40000 loss:  0.2986427450112795 params:  [-1.84300994  4.19719901 -3.88207703] gradients:  [0.02263589994401144, 0.0002204529089488953, 0.011447106923489957]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.84300994,  4.19719901, -3.88207703])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd = gradient_descent(X_train, y_train, learning_rate=0.01, max_iter=100000, tolerance=0.0001, optimizer=\"sgd\")\n",
    "new_param_sgd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Gradient Descent를 이용하면 시간이 오래 걸린다. (전체 데이터를 사용하기 때문이다.)  \n",
    "그러나 Stochastic Gradient Descent를 이용하면 시간은 짧게 걸린다. (랜덤하게 한 자료만을 골라서 사용하기 때문이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loss:  0.8706463644215906 params:  [0.41099396 0.60807901 0.40978645] gradients:  [0.3020330440664404, 0.0717445125619561, 0.2716870406752564]\n",
      "200 loss:  0.7290629590855446 params:  [0.13486991 0.55403978 0.16072047] gradients:  [0.2493185225078109, 0.035455429221407084, 0.22517120839450772]\n",
      "300 loss:  0.6374508466116798 params:  [-0.08890297  0.53618322 -0.04135887] gradients:  [0.19874235533605758, 0.0009646425724988842, 0.1795962278203253]\n",
      "400 loss:  0.5798860217138031 params:  [-0.26588802  0.54887709 -0.2020722 ] gradients:  [0.1564008929191305, -0.024971273029981395, 0.14331206646118258]\n",
      "500 loss:  0.5423423578989851 params:  [-0.40523545  0.58279954 -0.33185299] gradients:  [0.12343951731996146, -0.04166049499184284, 0.11759187642065191]\n",
      "600 loss:  0.5161362543207566 params:  [-0.51569471  0.6297537  -0.44018238] gradients:  [0.09838357006005281, -0.05140771869377538, 0.1000035187785643]\n",
      "700 loss:  0.49655257259757235 params:  [-0.60422709  0.6840393  -0.53381329] gradients:  [0.07935266993023825, -0.0566268981925297, 0.08785667913445822]\n",
      "800 loss:  0.48106688766763755 params:  [-0.67605462  0.7420417  -0.61715579] gradients:  [0.06479516633578311, -0.05904224152131047, 0.07920132207071612]\n",
      "900 loss:  0.4682801802356628 params:  [-0.73505021  0.80154439 -0.69303204] gradients:  [0.05355949467576251, -0.05975256660761121, 0.0727836093767086]\n",
      "1000 loss:  0.45737810620554614 params:  [-0.78410014  0.86120506 -0.7632626 ] gradients:  [0.044811044537165536, -0.05943505362008936, 0.0678235073679237]\n",
      "1100 loss:  0.44786207156779256 params:  [-0.82537564  0.92021835 -0.82904615] gradients:  [0.03794359400052475, -0.05850528876646767, 0.0638362512636433]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.82537564,  0.92021835, -0.82904615])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_bgd1 = gradient_descent(X_train, y_train, tolerance = 0.01) #tolerance를 높임\n",
    "new_param_bgd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop 조건이 완화되었기 때문에 tolerance값이 매우 작았을 때보다 (stop 조건이 더 강화되었을 때) 시간은 덜 소요되지만, 찾은 최적값이 global minimum loss일지는 정확하지 않다. (abs(new_loss-loss)가 0만큼의 작은 값으로 수렴하지는 않았기 때문이다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 loss:  0.30468900529745196 params:  [-1.74382893  4.46759366 -3.86873795] gradients:  [0.002799653575721985, -0.0045614120995812095, -0.0022775932011110897]\n",
      "20000 loss:  0.2998085400876687 params:  [-1.66886504  4.2212814  -3.9867748 ] gradients:  [0.005548435295077723, -0.006469902997431278, -0.0019519138525176077]\n",
      "30000 loss:  0.3036923565653971 params:  [-1.94846687  4.19322638 -3.66729919] gradients:  [0.014021397765032948, -0.023344338998658162, -0.017880902748647617]\n",
      "40000 loss:  0.3069402973883745 params:  [-2.02833891  4.65187149 -3.96165637] gradients:  [0.03053533911419363, -0.011670595133130833, -0.0026855477372689828]\n",
      "50000 loss:  0.30236305268367747 params:  [-1.88820146  4.47445718 -3.92241949] gradients:  [0.17525467083222973, 0.31392995213102726, 0.3429491619659808]\n",
      "60000 loss:  0.30368475861503696 params:  [-1.93901921  4.16930199 -4.30569713] gradients:  [0.044886664762745214, -0.028351098174998743, -0.015790920878440026]\n",
      "70000 loss:  0.3003845878171088 params:  [-1.66957686  4.31836182 -3.99699155] gradients:  [-0.08870881801411201, -0.10200843548586104, -0.0214550541802526]\n",
      "80000 loss:  0.2990670485235515 params:  [-1.9123813   4.24482085 -4.15280397] gradients:  [-0.04059475943052282, -0.029323823968722972, 0.019636441619087595]\n",
      "90000 loss:  0.29909650626842016 params:  [-1.72859725  4.26486366 -3.98777016] gradients:  [0.012208922741127065, -0.0033612049389086813, 0.004563482660436909]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-1.72859725,  4.26486366, -3.98777016])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_param_sgd1 = gradient_descent(X_train, y_train, learning_rate=0.05, max_iter=100000, tolerance=0.0001, optimizer=\"sgd\")\n",
    "new_param_sgd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learning_rate가 작았을 때와 결과값은 비슷하지만, gradients 값이 작아졌고 시행 횟수가 더 많아졌다. (학습률이 높아지면서 어디로 튈지 모르는 불안정성이 더 커졌기 때문인 것으로 생각된다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = []\n",
    "for i in range(len(y_test)):\n",
    "    p = logistic(X_test.iloc[i,:], new_param_bgd)\n",
    "    if p> 0.5 :\n",
    "        y_predict.append(1)\n",
    "    else :\n",
    "        y_predict.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[38,  2],\n",
       "       [ 2,  8]], dtype=int64)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "confusion_matrix(y_test, y_predict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
